{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999088948806\n"
     ]
    }
   ],
   "source": [
    "#Neuron\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class neuron:\n",
    "    \n",
    "    def __init__(self,weights,bias):\n",
    "        self.weights=weights\n",
    "        self.bias=bias\n",
    "    \n",
    "    def feedforward(self,x):\n",
    "        y=np.dot(self.weights,x)+self.bias\n",
    "        return(sigmoid(y))\n",
    "\n",
    "weight=np.array([0,1])\n",
    "x=np.array([2,3])\n",
    "bias=4\n",
    "n=neuron(weight,bias)\n",
    "print(n.feedforward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.721632560952\n"
     ]
    }
   ],
   "source": [
    "#Feedforward Neural Network\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class neuron:\n",
    "    \n",
    "    def __init__(self,weights,bias):\n",
    "        self.weights=weights\n",
    "        self.bias=bias\n",
    "    \n",
    "    def feedforward(self,x):\n",
    "        y=np.dot(self.weights,x)+self.bias\n",
    "        return(sigmoid(y))\n",
    "\n",
    "weight=np.array([0,1])\n",
    "x=np.array([2,3])\n",
    "bias=0\n",
    "h1=neuron(weight,bias)\n",
    "h2=neuron(weight,bias)\n",
    "o1=h1.feedforward(x)\n",
    "o2=h2.feedforward(x)\n",
    "ho=np.array([o1,o2])\n",
    "o=neuron(weight,bias)\n",
    "output=o.feedforward(ho)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.721632560952\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Feedforward Neural Network\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self,weights,bias):\n",
    "        self.weights=weights\n",
    "        self.bias=bias\n",
    "    def feedforward(self,x):\n",
    "        y=np.dot(self.weights,x)+self.bias\n",
    "        return sigmoid(y)\n",
    "    \n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights=np.array([0,1])\n",
    "        bias=0\n",
    "        self.h1=Neuron(weights,bias)\n",
    "        self.h2=Neuron(weights,bias)\n",
    "        self.ho=Neuron(weights,bias)\n",
    "        \n",
    "    def feedforwardNetwork(self,x):\n",
    "        o1=self.h1.feedforward(x)\n",
    "        o2=self.h2.feedforward(x)\n",
    "        o=self.ho.feedforward(np.array([o1,o2]))\n",
    "        return o\n",
    "    \n",
    "x=np.array([2,3])\n",
    "n=NeuralNetwork()\n",
    "output=n.feedforwardNetwork(x)\n",
    "print(output)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "## Calculating Mean Square Error\n",
    "import numpy as np\n",
    "def MSE(y_true,y_pred):\n",
    "    return ((y_true-y_pred)**2).mean()\n",
    "\n",
    "y_true=np.array([0,1,0,1])\n",
    "y_pred=np.array([0,0,0,0])\n",
    "print(MSE(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  0.38942680879\n",
      "Epoch:  0  Loss:  0.389006286784\n",
      "Epoch:  0  Loss:  0.388586272171\n",
      "Epoch:  0  Loss:  0.38827563886\n",
      "Epoch:  10  Loss:  0.376726380681\n",
      "Epoch:  10  Loss:  0.376306409098\n",
      "Epoch:  10  Loss:  0.375887221328\n",
      "Epoch:  10  Loss:  0.375583061923\n",
      "Epoch:  20  Loss:  0.364189357206\n",
      "Epoch:  20  Loss:  0.363772584125\n",
      "Epoch:  20  Loss:  0.363356869048\n",
      "Epoch:  20  Loss:  0.363058741521\n",
      "Epoch:  30  Loss:  0.351849220762\n",
      "Epoch:  30  Loss:  0.351438322066\n",
      "Epoch:  30  Loss:  0.351028747301\n",
      "Epoch:  30  Loss:  0.350736154832\n",
      "Epoch:  40  Loss:  0.339738904967\n",
      "Epoch:  40  Loss:  0.339336506097\n",
      "Epoch:  40  Loss:  0.338935682863\n",
      "Epoch:  40  Loss:  0.338648107568\n",
      "Epoch:  50  Loss:  0.327889880872\n",
      "Epoch:  50  Loss:  0.327498479326\n",
      "Epoch:  50  Loss:  0.327108885348\n",
      "Epoch:  50  Loss:  0.326825820067\n",
      "Epoch:  60  Loss:  0.316331257688\n",
      "Epoch:  60  Loss:  0.315953151\n",
      "Epoch:  60  Loss:  0.315577059199\n",
      "Epoch:  60  Loss:  0.315298035574\n",
      "Epoch:  70  Loss:  0.305088962509\n",
      "Epoch:  70  Loss:  0.30472618601\n",
      "Epoch:  70  Loss:  0.304365603245\n",
      "Epoch:  70  Loss:  0.304090214717\n",
      "Epoch:  80  Loss:  0.294185056521\n",
      "Epoch:  80  Loss:  0.293839334356\n",
      "Epoch:  80  Loss:  0.293495953588\n",
      "Epoch:  80  Loss:  0.293223872419\n",
      "Epoch:  90  Loss:  0.283637231985\n",
      "Epoch:  90  Loss:  0.283309943662\n",
      "Epoch:  90  Loss:  0.282985111746\n",
      "Epoch:  90  Loss:  0.282716099682\n",
      "Epoch:  100  Loss:  0.273458516698\n",
      "Epoch:  100  Loss:  0.273150680097\n",
      "Epoch:  100  Loss:  0.272845382027\n",
      "Epoch:  100  Loss:  0.272579294747\n",
      "Epoch:  110  Loss:  0.263657193353\n",
      "Epoch:  110  Loss:  0.263369463802\n",
      "Epoch:  110  Loss:  0.263084322926\n",
      "Epoch:  110  Loss:  0.262821108821\n",
      "Epoch:  120  Loss:  0.254236922829\n",
      "Epoch:  120  Loss:  0.25396960668\n",
      "Epoch:  120  Loss:  0.253704899248\n",
      "Epoch:  120  Loss:  0.253444593427\n",
      "Epoch:  130  Loss:  0.245197045168\n",
      "Epoch:  130  Loss:  0.244950125435\n",
      "Epoch:  130  Loss:  0.244705807015\n",
      "Epoch:  130  Loss:  0.244448521613\n",
      "Epoch:  140  Loss:  0.236533021315\n",
      "Epoch:  140  Loss:  0.23630619244\n",
      "Epoch:  140  Loss:  0.236081933215\n",
      "Epoch:  140  Loss:  0.235827845166\n",
      "Epoch:  150  Loss:  0.228236973203\n",
      "Epoch:  150  Loss:  0.228029681813\n",
      "Epoch:  150  Loss:  0.227824907655\n",
      "Epoch:  150  Loss:  0.227574245028\n",
      "Epoch:  160  Loss:  0.220298279077\n",
      "Epoch:  160  Loss:  0.220109767749\n",
      "Epoch:  160  Loss:  0.219923704095\n",
      "Epoch:  160  Loss:  0.219676732007\n",
      "Epoch:  170  Loss:  0.212704184351\n",
      "Epoch:  170  Loss:  0.212533535758\n",
      "Epoch:  170  Loss:  0.212365251681\n",
      "Epoch:  170  Loss:  0.212122258594\n",
      "Epoch:  180  Loss:  0.205440394421\n",
      "Epoch:  180  Loss:  0.20528657371\n",
      "Epoch:  180  Loss:  0.2051350241\n",
      "Epoch:  180  Loss:  0.204896309136\n",
      "Epoch:  190  Loss:  0.19849162351\n",
      "Epoch:  190  Loss:  0.19835351734\n",
      "Epoch:  190  Loss:  0.198217581646\n",
      "Epoch:  190  Loss:  0.197983443327\n",
      "Epoch:  200  Loss:  0.191842081666\n",
      "Epoch:  200  Loss:  0.191718532864\n",
      "Epoch:  200  Loss:  0.191597049392\n",
      "Epoch:  200  Loss:  0.191367776074\n",
      "Epoch:  210  Loss:  0.185475889571\n",
      "Epoch:  210  Loss:  0.185365726842\n",
      "Epoch:  210  Loss:  0.185257522082\n",
      "Epoch:  210  Loss:  0.18503338416\n",
      "Epoch:  220  Loss:  0.179377417265\n",
      "Epoch:  220  Loss:  0.179279479806\n",
      "Epoch:  220  Loss:  0.179183392666\n",
      "Epoch:  220  Loss:  0.178964636528\n",
      "Epoch:  230  Loss:  0.173531548043\n",
      "Epoch:  230  Loss:  0.173444705209\n",
      "Epoch:  230  Loss:  0.17335960631\n",
      "Epoch:  230  Loss:  0.17314644994\n",
      "Epoch:  240  Loss:  0.167923872487\n",
      "Epoch:  240  Loss:  0.167847038858\n",
      "Epoch:  240  Loss:  0.167771845271\n",
      "Epoch:  240  Loss:  0.167564475331\n",
      "Epoch:  250  Loss:  0.162540820018\n",
      "Epoch:  250  Loss:  0.162472966372\n",
      "Epoch:  250  Loss:  0.162406652288\n",
      "Epoch:  250  Loss:  0.162205222487\n",
      "Epoch:  260  Loss:  0.157369736679\n",
      "Epoch:  260  Loss:  0.157309897412\n",
      "Epoch:  260  Loss:  0.157251501306\n",
      "Epoch:  260  Loss:  0.157056131856\n",
      "Epoch:  270  Loss:  0.152398918301\n",
      "Epoch:  270  Loss:  0.152346195873\n",
      "Epoch:  270  Loss:  0.152294824731\n",
      "Epoch:  270  Loss:  0.152105602684\n",
      "Epoch:  280  Loss:  0.147617608025\n",
      "Epoch:  280  Loss:  0.147571174968\n",
      "Epoch:  280  Loss:  0.14752600612\n",
      "Epoch:  280  Loss:  0.147342986411\n",
      "Epoch:  290  Loss:  0.143015966504\n",
      "Epoch:  290  Loss:  0.142975065505\n",
      "Epoch:  290  Loss:  0.14293534656\n",
      "Epoch:  290  Loss:  0.142758553581\n",
      "Epoch:  300  Loss:  0.138585022262\n",
      "Epoch:  300  Loss:  0.138548964758\n",
      "Epoch:  300  Loss:  0.138514012088\n",
      "Epoch:  300  Loss:  0.138343441644\n",
      "Epoch:  310  Loss:  0.134316608628\n",
      "Epoch:  310  Loss:  0.134284772299\n",
      "Epoch:  310  Loss:  0.134253968446\n",
      "Epoch:  310  Loss:  0.134089589984\n",
      "Epoch:  320  Loss:  0.130203292654\n",
      "Epoch:  320  Loss:  0.130175118138\n",
      "Epoch:  320  Loss:  0.130147908465\n",
      "Epoch:  320  Loss:  0.129989667452\n",
      "Epoch:  330  Loss:  0.126238300383\n",
      "Epoch:  330  Loss:  0.126213287475\n",
      "Epoch:  330  Loss:  0.126189176319\n",
      "Epoch:  330  Loss:  0.126036996705\n",
      "Epoch:  340  Loss:  0.122415441924\n",
      "Epoch:  340  Loss:  0.122393145468\n",
      "Epoch:  340  Loss:  0.122371692\n",
      "Epoch:  340  Loss:  0.122225478686\n",
      "Epoch:  350  Loss:  0.118729038919\n",
      "Epoch:  350  Loss:  0.118709064561\n",
      "Epoch:  350  Loss:  0.118689878527\n",
      "Epoch:  350  Loss:  0.11854951979\n",
      "Epoch:  360  Loss:  0.115173856281\n",
      "Epoch:  360  Loss:  0.115155856217\n",
      "Epoch:  360  Loss:  0.11513859367\n",
      "Epoch:  360  Loss:  0.115003963513\n",
      "Epoch:  370  Loss:  0.111745039473\n",
      "Epoch:  370  Loss:  0.111728708278\n",
      "Epoch:  370  Loss:  0.111713067406\n",
      "Epoch:  370  Loss:  0.111584027787\n",
      "Epoch:  380  Loss:  0.108438058062\n",
      "Epoch:  380  Loss:  0.10842312867\n",
      "Epoch:  380  Loss:  0.108408845784\n",
      "Epoch:  380  Loss:  0.108285248715\n",
      "Epoch:  390  Loss:  0.105248655887\n",
      "Epoch:  390  Loss:  0.105234895779\n",
      "Epoch:  390  Loss:  0.105221741499\n",
      "Epoch:  390  Loss:  0.10510343101\n",
      "Epoch:  400  Loss:  0.102172807881\n",
      "Epoch:  400  Loss:  0.102160015498\n",
      "Epoch:  400  Loss:  0.102147791183\n",
      "Epoch:  400  Loss:  0.102034605116\n",
      "Epoch:  410  Loss:  0.0992066832987\n",
      "Epoch:  410  Loss:  0.099194684709\n",
      "Epoch:  410  Loss:  0.0991832191308\n",
      "Epoch:  410  Loss:  0.0990749907895\n",
      "Epoch:  420  Loss:  0.0963466149899\n",
      "Epoch:  420  Loss:  0.0963352608083\n",
      "Epoch:  420  Loss:  0.0963244070851\n",
      "Epoch:  420  Loss:  0.0962209667238\n",
      "Epoch:  430  Loss:  0.0935890741835\n",
      "Epoch:  430  Loss:  0.0935782367518\n",
      "Epoch:  430  Loss:  0.0935678695397\n",
      "Epoch:  430  Loss:  0.0934690456971\n",
      "Epoch:  440  Loss:  0.0909306502312\n",
      "Epoch:  440  Loss:  0.0909202210464\n",
      "Epoch:  440  Loss:  0.0909102339815\n",
      "Epoch:  440  Loss:  0.0908158546672\n",
      "Epoch:  450  Loss:  0.0883680346948\n",
      "Epoch:  450  Loss:  0.0883579220803\n",
      "Epoch:  450  Loss:  0.0883482254612\n",
      "Epoch:  450  Loss:  0.0882581191997\n",
      "Epoch:  460  Loss:  0.0858980091781\n",
      "Epoch:  460  Loss:  0.0858881361828\n",
      "Epoch:  460  Loss:  0.0858786548822\n",
      "Epoch:  460  Loss:  0.0857926516224\n",
      "Epoch:  470  Loss:  0.0835174363191\n",
      "Epoch:  470  Loss:  0.083507738832\n",
      "Epoch:  470  Loss:  0.0834984104226\n",
      "Epoch:  470  Loss:  0.0834163423215\n",
      "Epoch:  480  Loss:  0.0812232533974\n",
      "Epoch:  480  Loss:  0.0812136784614\n",
      "Epoch:  480  Loss:  0.0812044515406\n",
      "Epoch:  480  Loss:  0.0811261536306\n",
      "Epoch:  490  Loss:  0.0790124680562\n",
      "Epoch:  490  Loss:  0.0790029723671\n",
      "Epoch:  490  Loss:  0.078993805066\n",
      "Epoch:  490  Loss:  0.0789191158144\n",
      "Epoch:  500  Loss:  0.0768821556953\n",
      "Epoch:  500  Loss:  0.0768727042695\n",
      "Epoch:  500  Loss:  0.0768635629301\n",
      "Epoch:  500  Loss:  0.0767923247005\n",
      "Epoch:  510  Loss:  0.0748294581411\n",
      "Epoch:  510  Loss:  0.0748200231374\n",
      "Epoch:  510  Loss:  0.0748108811435\n",
      "Epoch:  510  Loss:  0.0747429405685\n",
      "Epoch:  520  Loss:  0.0728515832594\n",
      "Epoch:  520  Loss:  0.0728421429409\n",
      "Epoch:  520  Loss:  0.0728329796879\n",
      "Epoch:  520  Loss:  0.072768187962\n",
      "Epoch:  530  Loss:  0.0709458052266\n",
      "Epoch:  530  Loss:  0.0709363430486\n",
      "Epoch:  530  Loss:  0.0709271430387\n",
      "Epoch:  530  Loss:  0.0708653561395\n",
      "Epoch:  540  Loss:  0.0691094652241\n",
      "Epoch:  540  Loss:  0.0690999690355\n",
      "Epoch:  540  Loss:  0.0690907210865\n",
      "Epoch:  540  Loss:  0.0690317999326\n",
      "Epoch:  550  Loss:  0.0673399723677\n",
      "Epoch:  550  Loss:  0.0673304337137\n",
      "Epoch:  550  Loss:  0.0673211302675\n",
      "Epoch:  550  Loss:  0.0672649408213\n",
      "Epoch:  560  Loss:  0.0656348047193\n",
      "Epoch:  560  Loss:  0.0656252182346\n",
      "Epoch:  560  Loss:  0.065615854757\n",
      "Epoch:  560  Loss:  0.0655622680786\n",
      "Epoch:  570  Loss:  0.0639915102673\n",
      "Epoch:  570  Loss:  0.0639818731503\n",
      "Epoch:  570  Loss:  0.0639724476096\n",
      "Epoch:  570  Loss:  0.0639213398693\n",
      "Epoch:  580  Loss:  0.062407707789\n",
      "Epoch:  580  Loss:  0.0623980193465\n",
      "Epoch:  580  Loss:  0.0623885317638\n",
      "Epoch:  580  Loss:  0.062339784218\n",
      "Epoch:  590  Loss:  0.0608810875346\n",
      "Epoch:  590  Loss:  0.0608713487899\n",
      "Epoch:  590  Loss:  0.0608618008513\n",
      "Epoch:  590  Loss:  0.0608152997889\n",
      "Epoch:  600  Loss:  0.0594094116919\n",
      "Epoch:  600  Loss:  0.0593996250483\n",
      "Epoch:  600  Loss:  0.0593900197727\n",
      "Epoch:  600  Loss:  0.0593456564356\n",
      "Epoch:  610  Loss:  0.0579905146106\n",
      "Epoch:  610  Loss:  0.0579806835634\n",
      "Epoch:  610  Loss:  0.0579710250183\n",
      "Epoch:  610  Loss:  0.0579286955007\n",
      "Epoch:  620  Loss:  0.0566223027741\n",
      "Epoch:  620  Loss:  0.0566124316655\n",
      "Epoch:  620  Loss:  0.0566027247249\n",
      "Epoch:  620  Loss:  0.0565623298566\n",
      "Epoch:  630  Loss:  0.0553027545215\n",
      "Epoch:  630  Loss:  0.0552928483322\n",
      "Epoch:  630  Loss:  0.0552830984719\n",
      "Epoch:  630  Loss:  0.0552445436868\n",
      "Epoch:  640  Loss:  0.0540299195266\n",
      "Epoch:  640  Loss:  0.0540199837005\n",
      "Epoch:  640  Loss:  0.0540101968254\n",
      "Epoch:  640  Loss:  0.0539733920203\n",
      "Epoch:  650  Loss:  0.05280191805\n",
      "Epoch:  650  Loss:  0.0527919583466\n",
      "Epoch:  650  Loss:  0.0527821406453\n",
      "Epoch:  650  Loss:  0.0527470000313\n",
      "Epoch:  660  Loss:  0.0516169399809\n",
      "Epoch:  660  Loss:  0.0516069623519\n",
      "Epoch:  660  Loss:  0.0515971201767\n",
      "Epoch:  660  Loss:  0.0515635621242\n",
      "Epoch:  670  Loss:  0.0504732436901\n",
      "Epoch:  670  Loss:  0.0504632541777\n",
      "Epoch:  670  Loss:  0.0504533939441\n",
      "Epoch:  670  Loss:  0.0504213408254\n",
      "Epoch:  680  Loss:  0.0493691547158\n",
      "Epoch:  680  Loss:  0.0493591593685\n",
      "Epoch:  680  Loss:  0.0493492874737\n",
      "Epoch:  680  Loss:  0.0493186655034\n",
      "Epoch:  690  Loss:  0.0483030643052\n",
      "Epoch:  690  Loss:  0.0482930691093\n",
      "Epoch:  690  Loss:  0.0482831918655\n",
      "Epoch:  690  Loss:  0.0482539309407\n",
      "Epoch:  700  Loss:  0.0472734278325\n",
      "Epoch:  700  Loss:  0.0472634386576\n",
      "Epoch:  700  Loss:  0.0472535622386\n",
      "Epoch:  700  Loss:  0.0472255957803\n",
      "Epoch:  710  Loss:  0.0462787631174\n",
      "Epoch:  710  Loss:  0.0462687856721\n",
      "Epoch:  710  Loss:  0.0462589160713\n",
      "Epoch:  710  Loss:  0.0462321808677\n",
      "Epoch:  720  Loss:  0.0453176486606\n",
      "Epoch:  720  Loss:  0.0453076884589\n",
      "Epoch:  720  Loss:  0.0452978314568\n",
      "Epoch:  720  Loss:  0.0452722675095\n",
      "Epoch:  730  Loss:  0.0443887218188\n",
      "Epoch:  730  Loss:  0.0443787841543\n",
      "Epoch:  730  Loss:  0.0443689452939\n",
      "Epoch:  730  Loss:  0.0443444956673\n",
      "Epoch:  740  Loss:  0.0434906769341\n",
      "Epoch:  740  Loss:  0.0434807668608\n",
      "Epoch:  740  Loss:  0.0434709514299\n",
      "Epoch:  740  Loss:  0.0434475621054\n",
      "Epoch:  750  Loss:  0.0426222634338\n",
      "Epoch:  750  Loss:  0.0426123857532\n",
      "Epoch:  750  Loss:  0.0426025987726\n",
      "Epoch:  750  Loss:  0.0425802185062\n",
      "Epoch:  760  Loss:  0.0417822839158\n",
      "Epoch:  760  Loss:  0.0417724431686\n",
      "Epoch:  760  Loss:  0.0417626893844\n",
      "Epoch:  760  Loss:  0.0417412695701\n",
      "Epoch:  770  Loss:  0.0409695922304\n",
      "Epoch:  770  Loss:  0.0409597926923\n",
      "Epoch:  770  Loss:  0.0409500765732\n",
      "Epoch:  770  Loss:  0.0409295711102\n",
      "Epoch:  780  Loss:  0.0401830915713\n",
      "Epoch:  780  Loss:  0.0401733372517\n",
      "Epoch:  780  Loss:  0.0401636629886\n",
      "Epoch:  780  Loss:  0.0401440281546\n",
      "Epoch:  790  Loss:  0.0394217325831\n",
      "Epoch:  790  Loss:  0.0394120272272\n",
      "Epoch:  790  Loss:  0.0394023987355\n",
      "Epoch:  790  Loss:  0.0393835930651\n",
      "Epoch:  800  Loss:  0.0386845114951\n",
      "Epoch:  800  Loss:  0.0386748585877\n",
      "Epoch:  800  Loss:  0.0386652795123\n",
      "Epoch:  800  Loss:  0.0386472636795\n",
      "Epoch:  810  Loss:  0.0379704682877\n",
      "Epoch:  810  Loss:  0.0379608710588\n",
      "Epoch:  810  Loss:  0.0379513447802\n",
      "Epoch:  810  Loss:  0.0379340814875\n",
      "Epoch:  820  Loss:  0.037278684897\n",
      "Epoch:  820  Loss:  0.0372691463286\n",
      "Epoch:  820  Loss:  0.0372596759708\n",
      "Epoch:  820  Loss:  0.0372431298424\n",
      "Epoch:  830  Loss:  0.0366082834617\n",
      "Epoch:  830  Loss:  0.0365988062961\n",
      "Epoch:  830  Loss:  0.0365893947348\n",
      "Epoch:  830  Loss:  0.0365735322152\n",
      "Epoch:  840  Loss:  0.0359584246177\n",
      "Epoch:  840  Loss:  0.0359490113662\n",
      "Epoch:  840  Loss:  0.0359396612385\n",
      "Epoch:  840  Loss:  0.0359244504958\n",
      "Epoch:  850  Loss:  0.0353283058411\n",
      "Epoch:  850  Loss:  0.0353189587933\n",
      "Epoch:  850  Loss:  0.0353096725075\n",
      "Epoch:  850  Loss:  0.0352950833417\n",
      "Epoch:  860  Loss:  0.0347171598437\n",
      "Epoch:  860  Loss:  0.0347078810773\n",
      "Epoch:  860  Loss:  0.0346986608229\n",
      "Epoch:  860  Loss:  0.0346846645786\n",
      "Epoch:  870  Loss:  0.0341242530212\n",
      "Epoch:  870  Loss:  0.0341150444118\n",
      "Epoch:  870  Loss:  0.03410589217\n",
      "Epoch:  870  Loss:  0.0340924616536\n",
      "Epoch:  880  Loss:  0.0335488839557\n",
      "Epoch:  880  Loss:  0.0335397471868\n",
      "Epoch:  880  Loss:  0.033530664741\n",
      "Epoch:  880  Loss:  0.0335177741422\n",
      "Epoch:  890  Loss:  0.0329903819728\n",
      "Epoch:  890  Loss:  0.0329813185461\n",
      "Epoch:  890  Loss:  0.032972307492\n",
      "Epoch:  890  Loss:  0.0329599323093\n",
      "Epoch:  900  Loss:  0.0324481057533\n",
      "Epoch:  900  Loss:  0.0324391169982\n",
      "Epoch:  900  Loss:  0.0324301787542\n",
      "Epoch:  900  Loss:  0.0324182957247\n",
      "Epoch:  910  Loss:  0.0319214419996\n",
      "Epoch:  910  Loss:  0.031912529083\n",
      "Epoch:  910  Loss:  0.0319036649001\n",
      "Epoch:  910  Loss:  0.0318922519324\n",
      "Epoch:  920  Loss:  0.0314098041553\n",
      "Epoch:  920  Loss:  0.0314009680909\n",
      "Epoch:  920  Loss:  0.0313921790627\n",
      "Epoch:  920  Loss:  0.0313812151741\n",
      "Epoch:  930  Loss:  0.0309126311779\n",
      "Epoch:  930  Loss:  0.0309038728358\n",
      "Epoch:  930  Loss:  0.0308951599075\n",
      "Epoch:  930  Loss:  0.0308846251642\n",
      "Epoch:  940  Loss:  0.0304293863639\n",
      "Epoch:  940  Loss:  0.0304207064791\n",
      "Epoch:  940  Loss:  0.0304120704572\n",
      "Epoch:  940  Loss:  0.0304019459178\n",
      "Epoch:  950  Loss:  0.029959556224\n",
      "Epoch:  950  Loss:  0.029950955405\n",
      "Epoch:  950  Loss:  0.0299423969657\n",
      "Epoch:  950  Loss:  0.0299326646281\n",
      "Epoch:  960  Loss:  0.0295026494076\n",
      "Epoch:  960  Loss:  0.0294941281449\n",
      "Epoch:  960  Loss:  0.0294856478427\n",
      "Epoch:  960  Loss:  0.0294762905933\n",
      "Epoch:  970  Loss:  0.0290581956759\n",
      "Epoch:  970  Loss:  0.0290497543494\n",
      "Epoch:  970  Loss:  0.029041352625\n",
      "Epoch:  970  Loss:  0.0290323541915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  980  Loss:  0.0286257449202\n",
      "Epoch:  980  Loss:  0.0286173838071\n",
      "Epoch:  980  Loss:  0.0286090609953\n",
      "Epoch:  980  Loss:  0.0286004059016\n",
      "Epoch:  990  Loss:  0.0282048662263\n",
      "Epoch:  990  Loss:  0.0281965855081\n",
      "Epoch:  990  Loss:  0.028188341845\n",
      "Epoch:  990  Loss:  0.0281800153686\n"
     ]
    }
   ],
   "source": [
    "## A complete neural network\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x));\n",
    "\n",
    "def derive_sigmoid(x):\n",
    "    #Derivate of Sigmoid f'(x)=f(x)*(1-f(x))\n",
    "    fx=sigmoid(x)\n",
    "    return fx*(1-fx)\n",
    "    \n",
    "\n",
    "class neuron:\n",
    "    def __init__(self,weights,bias):\n",
    "        self.weights=weight\n",
    "        self.bias=bias\n",
    "        \n",
    "    def feedforward(self,x):\n",
    "        y=np.dot(self.weights,x)+self.bias\n",
    "        return y\n",
    "    \n",
    "def mse_loss(ypred,ytrue):\n",
    "    return ((ypred-ytrue)**2).mean()\n",
    "\n",
    "class NeuralNetwork:\n",
    "    ## A Neural network with 2 inputs, 1 hidden layer with 2 neurons(h1,h2) and one output layer with 1 neuron (o)\n",
    "        def __init__(self):\n",
    "            \n",
    "            #weights of the inputs to the neurons\n",
    "            self.w1=np.random.normal()\n",
    "            self.w2=np.random.normal()\n",
    "            self.w3=np.random.normal()\n",
    "            self.w4=np.random.normal()\n",
    "            self.w5=np.random.normal()\n",
    "            self.w6=np.random.normal()\n",
    "            \n",
    "            #biases\n",
    "            self.b1=np.random.normal()\n",
    "            self.b2=np.random.normal()\n",
    "            self.b3=np.random.normal()\n",
    "        \n",
    "        def feedforward(self,x):\n",
    "            h1=sigmoid(x[0]*self.w1+x[1]*self.w2+self.b1)\n",
    "            h2=sigmoid(x[0]*self.w3+x[1]*self.w4+self.b2)\n",
    "            o1=sigmoid(h1*self.w5+h2*self.w6+self.b3)\n",
    "            return o1\n",
    "        \n",
    "        def train(self,data,all_y_true):\n",
    "            ## data in nX2 numpy array , n= # of dataset\n",
    "            ## all_y_true is numpy array with n elements\n",
    "            \n",
    "            learn_rate=0.01\n",
    "            epochs=1000 ## number of time loop through the entire dataset\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                for x,ytrue in zip(data,all_y_true):\n",
    "                   # ypred=self.feedforward(x)\n",
    "                  #  loss=(ytrue-ypred)**2\n",
    "                    \n",
    "                    sum_h1=self.w1*x[0]+self.w2*x[1]+self.b1\n",
    "                    h1=sigmoid(sum_h1)\n",
    "                    sum_h2=self.w3*x[0]+self.w4*x[1]+self.b2\n",
    "                    h2=sigmoid(sum_h2)\n",
    "                    sum_ho=self.w5*h1+self.w6*h2+self.b2\n",
    "                    ho=sigmoid(sum_ho)\n",
    "                    ypred=ho\n",
    "                    \n",
    "                      # --- Calculate partial derivatives.\n",
    "                    # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "                    d_L_d_ypred=-2*(ytrue-ypred)\n",
    "                    \n",
    "                    #Neuron ho\n",
    "                    d_ypred_d_w5=h1*derive_sigmoid(sum_ho)\n",
    "                    d_ypred_d_w6=h2*derive_sigmoid(sum_ho)\n",
    "                    d_ypred_d_b3=derive_sigmoid(sum_ho)\n",
    "                    \n",
    "                    d_ypred_d_h1=self.w5*derive_sigmoid(sum_ho)\n",
    "                    d_ypred_d_h2=self.w6*derive_sigmoid(sum_ho)\n",
    "                    \n",
    "                    #Neuron h1\n",
    "                    d_h1_d_w1=x[0]*derive_sigmoid(sum_h1)\n",
    "                    d_h1_d_w2=x[1]*derive_sigmoid(sum_h1)\n",
    "                    d_h1_d_b1=derive_sigmoid(sum_h1)\n",
    "                    \n",
    "                    #Neuron h2\n",
    "                    d_h2_d_w3=x[0]*derive_sigmoid(sum_h2)\n",
    "                    d_h2_d_w4=x[1]*derive_sigmoid(sum_h2)\n",
    "                    d_h2_d_b2=derive_sigmoid(sum_h2)\n",
    "                    \n",
    "                    #Derivative of loss wrt to weights and biases\n",
    "                    d_L_d_w1=d_L_d_ypred*d_ypred_d_h1*d_h1_d_w1\n",
    "                    d_L_d_w2=d_L_d_ypred*d_ypred_d_h1*d_h1_d_w2\n",
    "                    d_L_d_w3=d_L_d_ypred*d_ypred_d_h2*d_h2_d_w3\n",
    "                    d_L_d_w4=d_L_d_ypred*d_ypred_d_h2*d_h2_d_w4\n",
    "                    d_L_d_w5=d_L_d_ypred*d_ypred_d_w5\n",
    "                    d_L_d_w6=d_L_d_ypred*d_ypred_d_w6\n",
    "                    d_L_d_b1=d_L_d_ypred*d_ypred_d_h1*d_h1_d_b1\n",
    "                    d_L_d_b2=d_L_d_ypred*d_ypred_d_h2*d_h2_d_b2\n",
    "                    d_L_d_b3=d_L_d_ypred*d_ypred_d_b3\n",
    "                    \n",
    "                    ##Update weights and biases\n",
    "                    \n",
    "                    ##Neuron h1\n",
    "                    self.w1=self.w1-learn_rate*d_L_d_w1\n",
    "                    self.w2=self.w2-learn_rate*d_L_d_w2\n",
    "                    self.b1=self.b1-learn_rate*d_L_d_b1\n",
    "                    \n",
    "                    ##Neuron h2\n",
    "                    self.w3=self.w3-learn_rate*d_L_d_w3\n",
    "                    self.w4=self.w4-learn_rate*d_L_d_w4\n",
    "                    self.b2=self.b2-learn_rate*d_L_d_b2\n",
    "                    \n",
    "                    ##Neuron ho\n",
    "                    self.w5=self.w5-learn_rate*d_L_d_w5\n",
    "                    self.w6=self.w6-learn_rate*d_L_d_w6\n",
    "                    self.b3=self.b3-learn_rate*d_L_d_b3\n",
    "                    \n",
    "                    \n",
    "                    ##Calculate total loss at the end of each epoch\n",
    "                    if epoch%10==0:\n",
    "                        y_preds=np.apply_along_axis(self.feedforward,1,data)\n",
    "                        loss=mse_loss(all_y_true,y_preds)\n",
    "                        print(\"Epoch: \",epoch,\" Loss: \",loss)\n",
    "\n",
    "#Define dataset\n",
    "data=np.array([\n",
    " [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "                    \n",
    "all_y_true =np.array([\n",
    "    1, #Alice\n",
    "    0, #Bob\n",
    "    0, #Charlie\n",
    "    1 #Diana\n",
    "])             \n",
    "\n",
    "\n",
    "#Train your network\n",
    "network=NeuralNetwork()\n",
    "network.train(data,all_y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dolly: 0.862\n",
      "Harshit: 0.164\n"
     ]
    }
   ],
   "source": [
    "##Use the model to make some predictions\n",
    "\n",
    "dolly=np.array([-7,-3])    # 128 pounds, 63 inches\n",
    "harshit=np.array([20,2])   # 155 pounds, 68 inches\n",
    "print(\"Dolly: %.3f\" % network.feedforward(dolly))\n",
    "print(\"Harshit: %.3f\" % network.feedforward(harshit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
